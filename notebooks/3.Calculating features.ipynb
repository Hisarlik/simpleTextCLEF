{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "from sacremoses import MosesTokenizer\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "import urllib\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "RESOURCES_DIR = Path(\"../resources\")\n",
    "DATASETS_PATH = RESOURCES_DIR / \"datasets\"\n",
    "WORD_EMBEDDINGS_NAME = \"glove.42B.300d\"\n",
    "DUMPS_DIR = RESOURCES_DIR / \"DUMPS\"\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ControlDivisionByZero(numerator, denominator):\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "\n",
    "class FeatureAbstract(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_ratio(self, kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Feature(FeatureAbstract):\n",
    "\n",
    "    def __init__(self, split, target_ratio):\n",
    "        self.split = split\n",
    "        self.target_ratio = target_ratio\n",
    "\n",
    "    def get_ratio(self, kwargs):\n",
    "        if not 'original_text_preprocessed' in kwargs:\n",
    "            kwargs['original_text_preprocessed'] = \"\"\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            simple_text = kwargs.get('simple_text')\n",
    "            original_text = kwargs.get('original_text')\n",
    "            result_ratio = self.calculate_ratio(simple_text, original_text)\n",
    "\n",
    "        elif self.split == \"valid\" or self.split == \"test\":\n",
    "            result_ratio = self.target_ratio\n",
    "        else:\n",
    "            raise ValueError(\"stage value not supported\")\n",
    "        kwargs['original_text_preprocessed'] += f'{self.name}_{result_ratio} '\n",
    "        return kwargs\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        name = \"\"\n",
    "        for word in re.findall('[A-Z][^A-Z]*', class_name):\n",
    "            if word: name += word[0]\n",
    "        if not name: name = class_name\n",
    "        return name\n",
    "\n",
    "\n",
    "class WordLengthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            self.tokenizer = MosesTokenizer(lang='en')\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        return round(ControlDivisionByZero(\n",
    "            len(self.tokenizer.tokenize(simple_text)),\n",
    "            len(self.tokenizer.tokenize(original_text))), 2)\n",
    "\n",
    "\n",
    "class CharLengthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        return round(ControlDivisionByZero(\n",
    "            len(simple_text),\n",
    "            len(original_text)), 2)\n",
    "\n",
    "\n",
    "class LevenshteinRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "        simple_text = word_tokenize(simple_text)\n",
    "        original_text = word_tokenize(original_text)\n",
    "        return round(Levenshtein.seqratio(original_text,\n",
    "                                       simple_text), 2)\n",
    "\n",
    "\n",
    "class DependencyTreeDepthRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            self.nlp = self.get_spacy_model()\n",
    "\n",
    "    def get_spacy_model(self):\n",
    "\n",
    "        model = 'en_core_web_sm'\n",
    "        if not spacy.util.is_package(model):\n",
    "            spacy.cli.download(model)\n",
    "            spacy.cli.link(model, model, force=True, model_path=spacy.util.get_package_path(model))\n",
    "        return spacy.load(model)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "\n",
    "        result_ratio = round(ControlDivisionByZero(\n",
    "            self.get_dependency_tree_depth(simple_text),\n",
    "            self.get_dependency_tree_depth(original_text)), 2)\n",
    "\n",
    "        return result_ratio\n",
    "\n",
    "    def get_dependency_tree_depth(self, sentence):\n",
    "\n",
    "        def get_subtree_depth(node):\n",
    "            if len(list(node.children)) == 0:\n",
    "                return 0\n",
    "            return 1 + max([get_subtree_depth(child) for child in node.children])\n",
    "\n",
    "        tree_depths = [get_subtree_depth(spacy_sentence.root) for spacy_sentence in self.nlp(sentence).sents]\n",
    "        if len(tree_depths) == 0:\n",
    "            return 0\n",
    "        return max(tree_depths)\n",
    "\n",
    "\n",
    "class WordRankRatio(Feature):\n",
    "\n",
    "    def __init__(self, stage, target_ratio):\n",
    "        super().__init__(stage, target_ratio)\n",
    "        if stage == \"train\":\n",
    "            self.tokenizer = MosesTokenizer(lang='en')\n",
    "            self.word2rank = self._get_word2rank()\n",
    "            self.length_rank = len(self.word2rank)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "\n",
    "        result_ratio = round(min(ControlDivisionByZero(self.get_lexical_complexity_score(simple_text),\n",
    "                                                       self.get_lexical_complexity_score(original_text)),\n",
    "                                 2), 2)\n",
    "\n",
    "        return result_ratio\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence, quantile_value=0.75):\n",
    "\n",
    "        words = self.tokenizer.tokenize(self._remove_stopwords(self._remove_punctuation(sentence)))\n",
    "        words = [word for word in words if word in self.word2rank]\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + self.length_rank)\n",
    "        return np.quantile([self._get_rank(word) for word in words], quantile_value)\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ' '.join([word for word in self.tokenizer.tokenize(text) if not self._is_punctuation(word)])\n",
    "\n",
    "    def _remove_stopwords(self, text):\n",
    "        return ' '.join([w for w in self.tokenizer.tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "    def _is_punctuation(self, word):\n",
    "        return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "    def _get_rank(self, word):\n",
    "        rank = self.word2rank.get(word, self.length_rank)\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "    def _get_word2rank(self, vocab_size=np.inf):\n",
    "        model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "        if model_filepath.exists():\n",
    "            with open(model_filepath, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            return model\n",
    "        else:\n",
    "            print(\"Downloading glove.42B.300d ...\")\n",
    "            self._download_glove(model_name='glove.42B.300d', dest_dir=str(DUMPS_DIR))\n",
    "            print(\"Preprocessing word2rank...\")\n",
    "            DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            lines_generator = self._yield_lines(WORD_EMBEDDINGS_PATH)\n",
    "            word2rank = {}\n",
    "            # next(lines_generator)\n",
    "            for i, line in enumerate(lines_generator):\n",
    "                if i >= vocab_size: break\n",
    "                word = line.split(' ')[0]\n",
    "                word2rank[word] = i\n",
    "\n",
    "            pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "            txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "            if txt_file.exists(): txt_file.unlink()\n",
    "            if zip_file.exists(): zip_file.unlink()\n",
    "            return word2rank\n",
    "\n",
    "    def _download_glove(self, model_name, dest_dir):\n",
    "        url = ''\n",
    "        if model_name == 'glove.6B':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        elif model_name == 'glove.42B.300d':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.42B.300d.zip'\n",
    "        elif model_name == 'glove.840B.300d':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "        elif model_name == 'glove.twitter.27B':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
    "        else:\n",
    "            possible_values = ['glove.6B', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B']\n",
    "            raise ValueError('Unknown model_name. Possible values are {}'.format(possible_values))\n",
    "        file_path = self._download_url(url, dest_dir)\n",
    "        out_filepath = Path(file_path)\n",
    "        out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "        # print(out_filepath, out_filepath.exists())\n",
    "        if not out_filepath.exists():\n",
    "            print(\"Extracting: \", Path(file_path).name)\n",
    "            self._unzip(file_path, dest_dir)\n",
    "\n",
    "    def _yield_lines(self, filepath):\n",
    "        filepath = Path(filepath)\n",
    "        with filepath.open('r', encoding=\"latin-1\") as f:\n",
    "            for line in f:\n",
    "                yield line.rstrip()\n",
    "\n",
    "    def _download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,\n",
    "                      desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path\n",
    "\n",
    "    def _unzip(self, file_path, dest_dir=None):\n",
    "        if dest_dir is None:\n",
    "            dest_dir = os.path.dirname(file_path)\n",
    "        if file_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dest_dir)\n",
    "        elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "            tar = tarfile.open(file_path, \"r:gz\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "        elif file_path.endswith(\"tar\"):\n",
    "            tar = tarfile.open(file_path, \"r:\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "\n",
    "    def _download_report_hook(self, t):\n",
    "        last_b = [0]\n",
    "\n",
    "        def inner(b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                t.total = tsize\n",
    "            t.update((b - last_b[0]) * bsize)\n",
    "            last_b[0] = b\n",
    "\n",
    "        return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences_pair = {\"simple_text\": \"esta es la version complicada\", \"original_text\": \"esta es la sencilla.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "char = CharLengthRatio(\"train\", 0.8)\n",
    "sentences_pair = char.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_length = WordLengthRatio(\"train\", 0.8)\n",
    "sentences_pair = word_length.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "levenshtein = LevenshteinRatio(\"train\", 0.8)\n",
    "sentences_pair = levenshtein.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dependency = DependencyTreeDepthRatio(\"train\", 0.8)\n",
    "sentences_pair = dependency.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_rank = WordRankRatio(\"train\", 0.8)\n",
    "sentences_pair = word_rank.get_ratio(sentences_pair)\n",
    "sentences_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Exploring features values from simpleText shared task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SIMPLETEXT_DATASET_PATH = DATASETS_PATH / \"simpleText/task 3\"\n",
    "SIMPLETEXT_TRAIN_PATH = SIMPLETEXT_DATASET_PATH / \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "complex_text = pd.read_csv(SIMPLETEXT_TRAIN_PATH / \"simpleText.train.complex.txt\", header=None, sep=\"\\t\", names=[\"original_text\"])\n",
    "simple_text = pd.read_csv(SIMPLETEXT_TRAIN_PATH / \"simpleText.train.simple.txt\", header=None, sep=\"\\t\",names=[\"simple_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences_pairs = pd.concat([complex_text, simple_text], axis=1)\n",
    "sentences_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i,row in sentences_pairs.iterrows():\n",
    "    sentences_pair = dict(original_text=row['original_text'], simple_text=row['simple_text'])\n",
    "    sentences_pair = char.get_ratio(sentences_pair)\n",
    "    sentences_pair = word_length.get_ratio(sentences_pair)\n",
    "    sentences_pair = levenshtein.get_ratio(sentences_pair)\n",
    "    sentences_pair = dependency.get_ratio(sentences_pair)\n",
    "    sentences_pair = word_rank.get_ratio(sentences_pair)\n",
    "    features = sentences_pair[\"original_text_preprocessed\"].strip().split(\" \")\n",
    "    instance = dict(feature.split(\"_\") for feature in features)\n",
    "    results.append(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(results)\n",
    "results_df = pd.DataFrame(results).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Character Length Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"CLR\"])\n",
    "fig.legend(labels=['Char Length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"WLR\"],label=\"Word Length\")\n",
    "fig.legend(labels=['Word Length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"LR\"])\n",
    "fig.legend(labels=['Levenshtein'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"LR\"])\n",
    "fig.legend(labels=['Levenshtein'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"DTDR\"])\n",
    "fig.legend(labels=['Deep Tree'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,6))\n",
    "sns.distplot(results_df[\"WRR\"])\n",
    "fig.legend(labels=['Word Rank'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}