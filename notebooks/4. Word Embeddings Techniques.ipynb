{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import re\n",
    "from sacremoses import MosesTokenizer\n",
    "import Levenshtein\n",
    "import spacy\n",
    "import nltk\n",
    "import pickle\n",
    "import urllib\n",
    "import os\n",
    "import tarfile\n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "RESOURCES_DIR = Path(\"C:/Users/Antonio/PycharmProjects/simpleTextCLEF/resources\")\n",
    "DATASETS_PATH = RESOURCES_DIR / \"datasets\"\n",
    "WORD_EMBEDDINGS_NAME = \"glove.42B.300d\"\n",
    "DUMPS_DIR = RESOURCES_DIR / \"DUMPS\"\n",
    "\n",
    "stopwords = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def ControlDivisionByZero(numerator, denominator):\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "class WordRankRatio():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tokenizer = MosesTokenizer(lang='en')\n",
    "        self.word2rank = self._get_word2rank()\n",
    "        self.length_rank = len(self.word2rank)\n",
    "\n",
    "    def calculate_ratio(self, simple_text, original_text):\n",
    "\n",
    "        result_ratio = round(min(ControlDivisionByZero(self.get_lexical_complexity_score(simple_text),\n",
    "                                                       self.get_lexical_complexity_score(original_text)),\n",
    "                                 2), 2)\n",
    "\n",
    "        return result_ratio\n",
    "\n",
    "    def get_lexical_complexity_score(self, sentence):\n",
    "\n",
    "        words = self.tokenizer.tokenize(self._remove_stopwords(self._remove_punctuation(sentence)))\n",
    "        words = [word for word in words if word in self.word2rank]\n",
    "        if len(words) == 0:\n",
    "            return np.log(1 + self.length_rank)\n",
    "        return np.quantile([self._get_rank(word) for word in words], 0.75)\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ' '.join([word for word in self.tokenizer.tokenize(text) if not self._is_punctuation(word)])\n",
    "\n",
    "    def _remove_stopwords(self, text):\n",
    "        return ' '.join([w for w in self.tokenizer.tokenize(text) if w.lower() not in stopwords])\n",
    "\n",
    "    def _is_punctuation(self, word):\n",
    "        return ''.join([char for char in word if char not in punctuation]) == ''\n",
    "\n",
    "    def _get_rank(self, word):\n",
    "        rank = self.word2rank.get(word, self.length_rank)\n",
    "        return np.log(1 + rank)\n",
    "\n",
    "    def _get_word2rank(self, vocab_size=np.inf):\n",
    "        model_filepath = DUMPS_DIR / f\"{WORD_EMBEDDINGS_NAME}.pk\"\n",
    "        if model_filepath.exists():\n",
    "            with open(model_filepath, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "            return model\n",
    "        else:\n",
    "            print(\"Downloading glove.42B.300d ...\")\n",
    "            self._download_glove(model_name='glove.42B.300d', dest_dir=str(DUMPS_DIR))\n",
    "            print(\"Preprocessing word2rank...\")\n",
    "            DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "            WORD_EMBEDDINGS_PATH = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            lines_generator = self._yield_lines(WORD_EMBEDDINGS_PATH)\n",
    "            word2rank = {}\n",
    "            # next(lines_generator)\n",
    "            for i, line in enumerate(lines_generator):\n",
    "                if i >= vocab_size: break\n",
    "                word = line.split(' ')[0]\n",
    "                word2rank[word] = i\n",
    "\n",
    "            pickle.dump(word2rank, open(model_filepath, 'wb'))\n",
    "            txt_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.txt'\n",
    "            zip_file = DUMPS_DIR / f'{WORD_EMBEDDINGS_NAME}.zip'\n",
    "            if txt_file.exists(): txt_file.unlink()\n",
    "            if zip_file.exists(): zip_file.unlink()\n",
    "            return word2rank\n",
    "\n",
    "    def _download_glove(self, model_name, dest_dir):\n",
    "        url = ''\n",
    "        if model_name == 'glove.6B':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "        elif model_name == 'glove.42B.300d':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.42B.300d.zip'\n",
    "        elif model_name == 'glove.840B.300d':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "        elif model_name == 'glove.twitter.27B':\n",
    "            url = 'http://nlp.stanford.edu/data/glove.twitter.27B.zip',\n",
    "        else:\n",
    "            possible_values = ['glove.6B', 'glove.42B.300d', 'glove.840B.300d', 'glove.twitter.27B']\n",
    "            raise ValueError('Unknown model_name. Possible values are {}'.format(possible_values))\n",
    "        file_path = self._download_url(url, dest_dir)\n",
    "        out_filepath = Path(file_path)\n",
    "        out_filepath = out_filepath.parent / f'{out_filepath.stem}.txt'\n",
    "        # print(out_filepath, out_filepath.exists())\n",
    "        if not out_filepath.exists():\n",
    "            print(\"Extracting: \", Path(file_path).name)\n",
    "            self._unzip(file_path, dest_dir)\n",
    "\n",
    "    def _yield_lines(self, filepath):\n",
    "        filepath = Path(filepath)\n",
    "        with filepath.open('r', encoding=\"latin-1\") as f:\n",
    "            for line in f:\n",
    "                yield line.rstrip()\n",
    "\n",
    "    def _download_url(self, url, output_path):\n",
    "        name = url.split('/')[-1]\n",
    "        file_path = f'{output_path}/{name}'\n",
    "        if not Path(file_path).exists():\n",
    "            with tqdm(unit='B', unit_scale=True, leave=True, miniters=1,\n",
    "                      desc=name) as t:  # all optional kwargs\n",
    "                urllib.request.urlretrieve(url, filename=file_path, reporthook=self._download_report_hook(t), data=None)\n",
    "        return file_path\n",
    "\n",
    "    def _unzip(self, file_path, dest_dir=None):\n",
    "        if dest_dir is None:\n",
    "            dest_dir = os.path.dirname(file_path)\n",
    "        if file_path.endswith('.zip'):\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(dest_dir)\n",
    "        elif file_path.endswith(\"tar.gz\") or file_path.endswith(\"tgz\"):\n",
    "            tar = tarfile.open(file_path, \"r:gz\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "        elif file_path.endswith(\"tar\"):\n",
    "            tar = tarfile.open(file_path, \"r:\")\n",
    "            tar.extractall(dest_dir)\n",
    "            tar.close()\n",
    "\n",
    "    def _download_report_hook(self, t):\n",
    "        last_b = [0]\n",
    "\n",
    "        def inner(b=1, bsize=1, tsize=None):\n",
    "            if tsize is not None:\n",
    "                t.total = tsize\n",
    "            t.update((b - last_b[0]) * bsize)\n",
    "            last_b[0] = b\n",
    "\n",
    "        return inner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "complex_sentence = \"We introduce Ignition: an end-to-end neural network architecture for training unconstrained self-driving vehicles in simulated environments.\"\n",
    "simple_sentence = \"Ignition is a neural network for training unconstrained self-driving vehicles in simulated environments.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "wordRank = WordRankRatio()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "9.92140252842767"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(complex_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "10.079523268944678"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRank.get_lexical_complexity_score(simple_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}